# Biased or Flawed? Biased or Flawed? Mitigating Stereotypes in Generative Language Models by Addressing Task-Specific Flaws

Recent studies have shown that generative language models often reflect and amplify societal biases in their outputs. However, these studies frequently conflate observed biases with other task-specific shortcomings, such as comprehension failure. For example, when a model misinterprets a text and produces a response that reinforces a stereotype, it becomes difficult to determine whether the issue arises from inherent bias or from a misunderstanding of the given content. In this paper, we conduct a multi-faceted evaluation that distinctly disentangles 'bias' from 'flaws' within the reading comprehension task. We leverage this nuanced understanding to propose a targeted stereotype mitigation framework that uses instruction-tuning to reduce stereotypical bias arising specifically from comprehension failures. We create an instruction-tuning dataset to mitigate stereotypes and evaluate several state-of-the-art generative models. We effectively reduce stereotypical outputs by over 60% across multiple dimensions -- including nationality, age, gender, disability, and physical appearance while maintaining their overall downstream utility. Our findings are generalizable to other tasks and highlight the need to critically disentangle the concept of `bias' from other types of errors to build more targeted and effective mitigation strategies.
